{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# TransformerCSSM: Iterative Attention for Visual Reasoning\n\nThis notebook introduces **TransformerCSSM**, a model that combines the parallelizability of state space models with the expressiveness of transformer-style attention.\n\n**Model Performance:** 88.50% accuracy on Pathfinder-14\n\n---\n\n## Table of Contents\n\n1. **What is CSSM?** - Convolutions in spectral domain, log-space stability\n2. **The Transformer's Trick** - Instant attention at all positions\n3. **When Iterative Attention Helps** - The Pathfinder task\n4. **CSSM as a Solution** - Parallel RNNs, but limited expressiveness\n5. **Insights from the hGRU** - Bilinear terms and growing receptive fields\n6. **TransformerCSSM** - Bringing it all together\n7. **Hands-On Analysis** - Gradients, mechanisms, and interpretability"
  },
  {
   "cell_type": "code",
   "source": "#@title Setup: Install dependencies and download checkpoint { display-mode: \"form\" }\nimport os\nimport sys\n\n# Check if running in Colab\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    # Install JAX with GPU support\n    !pip install -q jax[cuda12] flax optax tensorflow\n    \n    # Clone CSSM repo\n    if not os.path.exists('CSSM'):\n        !git clone https://github.com/your-repo/CSSM.git\n        sys.path.insert(0, 'CSSM')\n    \n    # Download checkpoint (--no-check-certificate for expired SSL cert)\n    CHECKPOINT_URL = \"https://connectomics.clps.brown.edu/tf_records/transformer_cssm_kqv64_epoch20.pkl\"\n    CHECKPOINT_PATH = \"transformer_cssm_checkpoint.pkl\"\n    \n    if not os.path.exists(CHECKPOINT_PATH):\n        print(f\"Downloading checkpoint from {CHECKPOINT_URL}...\")\n        !wget -q --no-check-certificate {CHECKPOINT_URL} -O {CHECKPOINT_PATH}\n        print(\"Download complete!\")\nelse:\n    # Local paths\n    CHECKPOINT_PATH = \"checkpoints/KQV_64/epoch_20/checkpoint.pkl\"\n\nprint(f\"Checkpoint path: {CHECKPOINT_PATH}\")\nprint(f\"Running in {'Colab' if IN_COLAB else 'local'} environment\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# Part I: Background\n---\n\n## 1. What is CSSM?\n\n**CSSM (Cepstral State Space Model)** is a recurrent neural network designed for efficient visual processing. Think of it as an RNN that can run in parallel.\n\n### The Core Idea\n\nA standard RNN updates its hidden state like this:\n\n$$h_{t+1} = A \\cdot h_t + B \\cdot x_t$$\n\nThe problem? Each timestep depends on the previous one, so you must compute them **sequentially**. For T timesteps, that's O(T) serial operations—slow on GPUs that thrive on parallelism.\n\n### Why Spectral Domain?\n\nCSSM performs spatial convolutions using the **Fast Fourier Transform (FFT)**:\n\n$$\\text{Conv}(K, X) = \\mathcal{F}^{-1}\\left(\\mathcal{F}(K) \\odot \\mathcal{F}(X)\\right)$$\n\n**In plain English:** Instead of sliding a kernel across an image (expensive), we:\n1. Transform both kernel and image to frequency domain (FFT)\n2. Multiply them element-wise (cheap!)\n3. Transform back (inverse FFT)\n\nThis reduces spatial convolution from O(N²) to O(N log N).\n\n### Why Log-Space (GOOM)?\n\nWhen you multiply many numbers together over time (like decay rates), values can explode or vanish:\n\n$$h_T = \\lambda^T \\cdot h_0 \\quad \\text{(exponential growth/decay)}$$\n\n**GOOM (Generalized Order of Magnitude)** solves this by working in log-space:\n\n$$\\log(a \\cdot b) = \\log(a) + \\log(b)$$\n\nMultiplications become additions, keeping values numerically stable even over hundreds of timesteps."
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 2. The Transformer's Trick: Instant Attention\n\nTransformers revolutionized deep learning with **self-attention**:\n\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right) V$$\n\n### What This Means\n\nEvery position in the input can instantly \"attend to\" every other position:\n- **Q (Query)**: \"What am I looking for?\"\n- **K (Key)**: \"What do I contain?\"\n- **V (Value)**: \"What information do I carry?\"\n\nThe $QK^T$ product computes similarity between all pairs of positions **in one shot**.\n\n### The Good\n\n- **Fully parallelizable**: No sequential dependencies\n- **Global context**: Any position can see any other position\n- **Scales well**: GPUs love matrix multiplications\n\n### The Catch\n\n- **O(N²) complexity**: Comparing all pairs of N positions is expensive\n- **All-or-nothing**: Attention is computed once, no refinement\n- **No iteration**: Can't \"change your mind\" or backtrack"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. When Iterative Attention Helps: The Pathfinder Task\n\nSome visual tasks require **iterative reasoning**—you can't solve them in one glance.\n\n### The Pathfinder Challenge\n\nThe task: **Are the two dots connected by a continuous curve?**\n\nThe image contains:\n- Two marker dots (endpoints)\n- A potential connecting contour (curved path)\n- Distractor curves (noise to confuse you)\n\n**Why is this hard?**\n- The contour can be long and winding\n- Distractors look similar to the real path\n- You need to \"trace\" the curve step by step\n\n### Why Iterative Attention?\n\nImagine tracing a contour with your finger:\n\n1. **Start at one dot**\n2. **Follow the curve locally** (look at nearby pixels)\n3. **Keep extending** your search\n4. **Backtrack if stuck** (hit a dead end? try another direction)\n5. **Succeed when you reach the other dot**\n\nThis is fundamentally **iterative**—you make local decisions that accumulate into a global answer.\n\n### The Transformer Problem\n\nA standard transformer computes attention once and makes a decision. But for Pathfinder:\n- **Early layers** can only see local structure\n- **To trace a long curve**, you need many layers stacked (deep = slow)\n- **No backtracking**: if an early layer makes a mistake, later layers can't fix it\n\n### The RNN Solution (and its flaw)\n\nAn RNN can iterate naturally:\n- Each timestep refines the answer\n- Information spreads gradually across the image\n- More timesteps = larger effective receptive field\n\n**But**: Traditional RNNs are sequential → O(T) time → slow on GPUs."
  },
  {
   "cell_type": "code",
   "source": "# Let's visualize the Pathfinder task\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# We'll load actual examples later, but here's what the task looks like:\nprint(\"=\"*60)\nprint(\"THE PATHFINDER TASK\")\nprint(\"=\"*60)\nprint()\nprint(\"  ┌─────────────────────────────────┐\")\nprint(\"  │     ●                           │  ● = marker dots\")\nprint(\"  │      ╲                          │\")\nprint(\"  │       ╲    ╱╲                   │  The question:\")\nprint(\"  │        ╲  ╱  ╲                  │  Are the two dots\")\nprint(\"  │         ╲╱    ╲                 │  connected by a\")\nprint(\"  │                 ╲    ╱╲         │  continuous curve?\")\nprint(\"  │                  ╲  ╱  ╲        │\")\nprint(\"  │    ╱╲             ╲╱    ●       │\")\nprint(\"  │   ╱  ╲                          │  (ignore distractors)\")\nprint(\"  │  ╱    ╲                         │\")\nprint(\"  └─────────────────────────────────┘\")\nprint()\nprint(\"Connected (positive): The curve links both dots\")\nprint(\"Disconnected (negative): Dots are on separate curves\")\nprint()\nprint(\"Difficulty levels:\")\nprint(\"  • Level 9:  Short contours (easy)\")\nprint(\"  • Level 14: Medium contours (this notebook)\")\nprint(\"  • Level 20: Long, winding contours (hard)\")\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 4. CSSM: A Parallel RNN (But Limited)\n\nCSSM offers a potential solution: **an RNN that runs in parallel**.\n\n### The Associative Scan Trick\n\nFor linear recurrences of the form:\n\n$$h_{t+1} = A \\cdot h_t + b_t$$\n\nWe can use the **associative scan** to compute all timesteps in O(log T) parallel time instead of O(T) sequential time.\n\n**How?** The operation $(A_2, b_2) \\circ (A_1, b_1) = (A_2 A_1, A_2 b_1 + b_2)$ is associative, so we can reorganize the computation into a tree:\n\n```\nt=0    t=1    t=2    t=3    t=4    t=5    t=6    t=7\n  \\    /        \\    /        \\    /        \\    /\n   [0:1]         [2:3]         [4:5]         [6:7]      ← Level 1\n      \\          /                \\          /\n       [0:3]                       [4:7]                ← Level 2\n           \\                      /\n                  [0:7]                                 ← Level 3\n```\n\nInstead of 8 sequential steps, we need only 3 parallel levels.\n\n### The Limitation\n\nBasic CSSM computes attention as a function of a **single state variable**:\n\n$$h_{t+1} = \\lambda \\cdot h_t + \\text{input}$$\n\nThis is like a transformer with only **one** of Q, K, or V—severely limited expressiveness.\n\n**Can we make a CSSM that benefits from the Query-Key interaction of transformers?**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Insights from the hGRU\n\nA key hint came from the **hGRU (horizontal Gated Recurrent Unit)**—an RNN we developed that successfully solves Pathfinder.\n\n### The hGRU's Secret: Bilinear Interactions\n\nThe hGRU has two interacting cell populations:\n- **Excitatory cells (X)**: Spread activation along contours\n- **Inhibitory cells (Y)**: Suppress distractors\n\nThe critical insight is the **bilinear term**:\n\n$$Y_{t+1} \\propto X_t \\odot Y_t$$\n\n**In plain English:** Inhibition is computed as the **product** of excitatory and inhibitory activity. This multiplicative interaction is much more expressive than simple addition.\n\n### Growing Receptive Fields = Growing Attention\n\nEach timestep, information spreads through a spatial kernel. After T timesteps:\n\n- **t=1**: Each pixel sees its immediate neighbors\n- **t=4**: Each pixel sees a moderate neighborhood  \n- **t=8**: Each pixel sees a large region\n\nThe kernel **compounds over time**, creating an effective receptive field that grows with each iteration.\n\n**This is like attention with a growing radius!**\n\nAt early timesteps, comparisons are local. At later timesteps, comparisons span the entire image. This allows the network to:\n1. **Start with local edge detection**\n2. **Gradually integrate** into longer contours\n3. **Make global decisions** only when enough context is gathered\n\n### The Challenge\n\nThe hGRU works great, but it's a traditional RNN—**sequential and slow**.\n\n**Can we preserve these bilinear, growing-receptive-field dynamics in a parallelizable CSSM?**"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n# Part II: TransformerCSSM\n---\n\n## 6. TransformerCSSM: Bringing It Together\n\n**TransformerCSSM** is our attempt to combine:\n- ✅ **Parallel computation** (from CSSM's associative scan)\n- ✅ **Query-Key interactions** (from transformers)\n- ✅ **Growing receptive fields** (from hGRU)\n- ✅ **Iterative refinement** (from RNNs)\n\n### The Three States\n\nWe use transformer-inspired naming for three interacting state variables:\n\n| State | Name | Role | Intuition |\n|-------|------|------|-----------|\n| **Q** | Query | \"What am I looking for?\" | Current representation seeking context |\n| **K** | Key | \"What do I contain?\" | Information available to match against |\n| **A** | Attention | \"What have I found?\" | Accumulated Q-K correlations over time |\n\n### The Update Equations (ELI5 Version)\n\n**Query Update:**\n$$Q_{t+1} = \\underbrace{\\lambda_Q \\cdot Q_t}_{\\text{remember old Q}} + \\underbrace{w \\cdot (\\mathcal{K} * K_t)}_{\\text{look at K through kernel}} + \\underbrace{\\alpha \\cdot (\\mathcal{K} * A_t)}_{\\text{attention feedback}} + \\underbrace{U_Q}_{\\text{new input}}$$\n\n*\"The Query remembers itself, looks at what the Key contains (through a spatial kernel), gets modulated by accumulated Attention, and receives new input.\"*\n\n**Key Update:**\n$$K_{t+1} = \\underbrace{w \\cdot (\\mathcal{K} * Q_t)}_{\\text{look at Q through kernel}} + \\underbrace{\\lambda_K \\cdot K_t}_{\\text{remember old K}} + \\underbrace{U_K}_{\\text{new input}}$$\n\n*\"The Key looks at what the Query is seeking (symmetric to above!) and remembers itself.\"*\n\n**Attention Accumulator:**\n$$A_{t+1} = \\underbrace{\\gamma \\cdot (Q_t + K_t)}_{\\text{accumulate Q-K activity}} + \\underbrace{\\lambda_A \\cdot A_t}_{\\text{remember old A}} + \\underbrace{U_A}_{\\text{new input}}$$\n\n*\"Attention accumulates the sum of Q and K over time—building up a record of where Q and K agreed.\"*\n\n### Why This Works\n\n1. **Q and K interact symmetrically** through a shared spatial kernel $\\mathcal{K}$\n2. **The kernel grows effective receptive field** over timesteps (like hGRU)\n3. **A accumulates Q-K correlation** over time (like building attention weights)\n4. **A feeds back into Q** (the accumulated attention modulates future queries)\n5. **Everything is linear** → can use associative scan → **parallel!**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Matrix form visualization\nprint(\"=\"*70)\nprint(\"TransformerCSSM as a 3x3 State Transition Matrix\")\nprint(\"=\"*70)\nprint()\nprint(\"  ┌                              ┐   ┌   ┐     ┌     ┐\")\nprint(\"  │  λ_Q      w·K      α·K       │   │ Q │     │ U_Q │\")\nprint(\"  │  w·K      λ_K       0        │ × │ K │  +  │ U_K │\")\nprint(\"  │   γ        γ       λ_A       │   │ A │     │ U_A │\")\nprint(\"  └                              ┘   └   ┘     └     ┘\")\nprint()\nprint(\"Where:\")\nprint(\"  • λ_Q, λ_K, λ_A = decay rates (memory)\")\nprint(\"  • w = Q↔K coupling weight (symmetric!)\")\nprint(\"  • α = attention feedback strength (A → Q)\")\nprint(\"  • γ = attention accumulation rate\")\nprint(\"  • K = spatial convolution kernel (via FFT)\")\nprint()\nprint(\"Key insight: Q↔K coupling is SYMMETRIC (same w in both directions)\")\nprint(\"This is like Q and K 'talking to each other' through the same channel\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### Sequential vs Parallel: The Best of Both Worlds\n\n**Sequential (traditional RNN):**\n```\nfor t in range(T):\n    Q[t+1] = λ_Q·Q[t] + w·(K * K[t]) + α·(K * A[t]) + U\n    K[t+1] = w·(K * Q[t]) + λ_K·K[t] + U  \n    A[t+1] = γ·(Q[t] + K[t]) + λ_A·A[t] + U\n```\nTime: **O(T)** sequential steps\n\n**Parallel (associative scan):**\n```\n# Compute all timesteps simultaneously using tree reduction\nstates = associative_scan(combine_fn, inputs)\n```\nTime: **O(log T)** parallel steps\n\nFor T=8 timesteps: sequential needs 8 steps, parallel needs only 3!\n\n---\n# Part III: Hands-On Analysis\n---\n\nNow let's load a trained TransformerCSSM and see how it solves Pathfinder."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Loading the Model\n\nWe'll load a TransformerCSSM trained on Pathfinder-14 (88.50% accuracy) and analyze:\n1. What the learned spatial kernel looks like\n2. How the model makes decisions over time\n3. Which mechanisms matter most"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pickle\nimport matplotlib.pyplot as plt\n\n# Load checkpoint (uses CHECKPOINT_PATH from setup cell)\nwith open(CHECKPOINT_PATH, 'rb') as f:\n    ckpt = pickle.load(f)\n\nparams = ckpt['params']\nprint(f\"Loaded checkpoint from epoch {ckpt.get('epoch', 'unknown')}\")\nprint(f\"CSSM params: {list(params['cssm_0'].keys())}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract TransformerCSSM parameters\ncssm_params = params['cssm_0']\n\n# Single spatial kernel (NOT two like HGRUBi)\nK_kernel = jnp.array(cssm_params['kernel'])  # (C, k, k)\nprint(f\"Spatial kernel shape: {K_kernel.shape}\")\n\n# Extract gate values - handle Dense layer dict structure\ndef get_gate(name):\n    \"\"\"Extract gate value, handling Dense layer dict format.\"\"\"\n    gate_data = cssm_params[name]\n    # TransformerCSSM stores gates as Dense layers: {'kernel': ..., 'bias': ...}\n    if isinstance(gate_data, dict):\n        kernel = gate_data['kernel']\n        bias = gate_data.get('bias', 0)\n        val = kernel.mean() + (bias.mean() if hasattr(bias, 'mean') else bias)\n    else:\n        val = gate_data.mean()\n    return float(jax.nn.sigmoid(val))\n\ngates = {\n    'lambda_Q': get_gate('decay_Q'),\n    'lambda_K': get_gate('decay_K'),\n    'lambda_A': get_gate('decay_A'),\n    'w': get_gate('w_qk'),\n    'alpha': get_gate('alpha'),\n    'gamma': get_gate('gamma'),\n}\n\nprint(\"\\nExtracted gate values:\")\nfor name, val in gates.items():\n    print(f\"  {name:>10}: {val:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize the single kernel (64 channels for KQV_64 model)\nfig, axes = plt.subplots(4, 8, figsize=(16, 8))\n\nK_np = np.array(K_kernel)\nvmax = np.abs(K_np).max()\n\nfor row in range(4):\n    for col in range(8):\n        idx = row * 8 + col\n        if idx < K_np.shape[0]:\n            axes[row, col].imshow(K_np[idx], cmap='RdBu_r', vmin=-vmax, vmax=vmax)\n            axes[row, col].set_title(f'K[{idx}]', fontsize=8)\n        axes[row, col].axis('off')\n\nplt.suptitle('TransformerCSSM: Single Spatial Kernel (64 channels, shared for Q↔K and A→Q)', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n# Mean kernel\nfig, ax = plt.subplots(figsize=(5, 4))\nim = ax.imshow(K_np.mean(axis=0), cmap='RdBu_r')\nax.set_title('Mean Kernel (averaged over 64 channels)', fontsize=12)\nax.axis('off')\nplt.colorbar(im, ax=ax, fraction=0.046)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load model for gradient computation\nfrom src.models.simple_cssm import SimpleCSSM\n\nmodel = SimpleCSSM(\n    num_classes=2,\n    embed_dim=64,  # 64-dim model (KQV_64)\n    depth=1,\n    cssm_type='transformer',  # TransformerCSSM!\n    kernel_size=15,\n    pos_embed='spatiotemporal',\n    seq_len=8,\n)\nprint(\"TransformerCSSM model loaded (embed_dim=64).\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load sample Pathfinder images\nimport tensorflow as tf\ntf.config.set_visible_devices([], 'GPU')\n\n# For Colab, we'll use a sample image URL; locally, load from TFRecords\nif IN_COLAB:\n    # Download sample images for Colab\n    import urllib.request\n    SAMPLE_URL = \"https://connectomics.clps.brown.edu/tf_records/pathfinder_samples.npz\"\n    try:\n        urllib.request.urlretrieve(SAMPLE_URL, \"pathfinder_samples.npz\")\n        data = np.load(\"pathfinder_samples.npz\")\n        pos_img, neg_img = data['pos'], data['neg']\n    except:\n        # Fallback: create dummy images for demo\n        print(\"Could not load samples, using random images for demo\")\n        pos_img = np.random.rand(224, 224, 3).astype(np.float32)\n        neg_img = np.random.rand(224, 224, 3).astype(np.float32)\nelse:\n    TFRECORD_DIR = '/home/dlinsley/pathfinder_tfrecord/difficulty_14/val'\n    \n    def parse_example(example):\n        features = tf.io.parse_single_example(example, {\n            'image': tf.io.FixedLenFeature([], tf.string),\n            'label': tf.io.FixedLenFeature([], tf.int64),\n        })\n        image = tf.io.decode_raw(features['image'], tf.float32)\n        image = tf.reshape(image, [224, 224, 3])\n        return image, features['label']\n    \n    val_files = sorted(tf.io.gfile.glob(f'{TFRECORD_DIR}/*.tfrecord'))\n    ds = tf.data.TFRecordDataset(val_files[:1]).map(parse_example)\n    \n    pos_img, neg_img = None, None\n    for img, label in ds:\n        if label.numpy() == 1 and pos_img is None:\n            pos_img = img.numpy()\n        elif label.numpy() == 0 and neg_img is None:\n            neg_img = img.numpy()\n        if pos_img is not None and neg_img is not None:\n            break\n\n# Visualize the examples\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\naxes[0].imshow(pos_img)\naxes[0].set_title('CONNECTED (Positive)\\nThe two dots ARE linked', fontsize=12)\naxes[0].axis('off')\n\naxes[1].imshow(neg_img)\naxes[1].set_title('DISCONNECTED (Negative)\\nThe two dots are NOT linked', fontsize=12)\naxes[1].axis('off')\n\nplt.suptitle('Pathfinder-14 Examples: Can You Trace the Contour?', fontsize=14)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Image shape: {pos_img.shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify predictions\n",
    "def forward_single(img):\n",
    "    x = jnp.array(img)[None, ...]\n",
    "    x_temporal = jnp.repeat(x[:, None, ...], 8, axis=1)\n",
    "    logits = model.apply({'params': params}, x_temporal, training=False)\n",
    "    return logits[0]\n",
    "\n",
    "pos_logits = forward_single(pos_img)\n",
    "neg_logits = forward_single(neg_img)\n",
    "\n",
    "print(\"TransformerCSSM Predictions:\")\n",
    "print(f\"  Positive: {pos_logits} → {'Connected' if pos_logits.argmax() == 1 else 'Disconnected'}\")\n",
    "print(f\"  Negative: {neg_logits} → {'Connected' if neg_logits.argmax() == 1 else 'Disconnected'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Temporal Gradient Attribution\n\nHow does the model's decision depend on each timestep? We compute gradients of the output with respect to the input at each of the 8 timesteps."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_temporal_gradients(img, target_class):\n",
    "    x = jnp.array(img)[None, ...]\n",
    "    x_temporal = jnp.repeat(x[:, None, ...], 8, axis=1)\n",
    "    \n",
    "    def forward_fn(x_t):\n",
    "        logits = model.apply({'params': params}, x_t, training=False)\n",
    "        return logits[0, target_class]\n",
    "    \n",
    "    grads = jax.grad(forward_fn)(x_temporal)\n",
    "    grad_magnitude = jnp.abs(grads).sum(axis=(0, 2, 3, 4))\n",
    "    spatial_grads = jnp.abs(grads[0]).sum(axis=-1)\n",
    "    return grad_magnitude, spatial_grads\n",
    "\n",
    "pos_grad_mag, pos_spatial = compute_temporal_gradients(pos_img, 1)\n",
    "neg_grad_mag, neg_spatial = compute_temporal_gradients(neg_img, 0)\n",
    "\n",
    "print(\"Gradient magnitude per timestep:\")\n",
    "print(f\"  Positive: {np.array(pos_grad_mag).round(2)}\")\n",
    "print(f\"  Negative: {np.array(neg_grad_mag).round(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot temporal importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "timesteps = np.arange(8)\n",
    "width = 0.35\n",
    "axes[0].bar(timesteps - width/2, np.array(pos_grad_mag), width, \n",
    "           label='Positive', color='green', alpha=0.7)\n",
    "axes[0].bar(timesteps + width/2, np.array(neg_grad_mag), width,\n",
    "           label='Negative', color='red', alpha=0.7)\n",
    "axes[0].set_xlabel('Timestep', fontsize=12)\n",
    "axes[0].set_ylabel('Gradient Magnitude', fontsize=12)\n",
    "axes[0].set_title('TransformerCSSM: Temporal Importance', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].set_xticks(timesteps)\n",
    "\n",
    "# Cumulative\n",
    "pos_cumsum = np.cumsum(np.array(pos_grad_mag))\n",
    "neg_cumsum = np.cumsum(np.array(neg_grad_mag))\n",
    "axes[1].plot(timesteps, pos_cumsum / pos_cumsum[-1], 'g-o', label='Positive', linewidth=2)\n",
    "axes[1].plot(timesteps, neg_cumsum / neg_cumsum[-1], 'r-o', label='Negative', linewidth=2)\n",
    "axes[1].set_xlabel('Timestep', fontsize=12)\n",
    "axes[1].set_ylabel('Cumulative Importance', fontsize=12)\n",
    "axes[1].set_title('Information Integration Over Time', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].set_xticks(timesteps)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Mechanism Attribution: Which Components Matter?\n\nWhich parts of the TransformerCSSM are most important for the decision? We compute gradients with respect to each learned parameter."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_gradients(img, target_class):\n",
    "    x = jnp.array(img)[None, ...]\n",
    "    x_temporal = jnp.repeat(x[:, None, ...], 8, axis=1)\n",
    "    \n",
    "    def loss_fn(p):\n",
    "        logits = model.apply({'params': p}, x_temporal, training=False)\n",
    "        return logits[0, target_class]\n",
    "    \n",
    "    return jax.grad(loss_fn)(params)\n",
    "\n",
    "pos_param_grads = parameter_gradients(pos_img, 1)\n",
    "neg_param_grads = parameter_gradients(neg_img, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransformerCSSM-specific gates\n",
    "mechanisms = [\n",
    "    ('w_qk', 'w (Q↔K coupling)', 'Symmetric Q-K interaction'),\n",
    "    ('alpha', 'α (A→Q feedback)', 'Attention feeds back to Q'),\n",
    "    ('gamma', 'γ (Q,K→A accum)', 'Attention accumulation rate'),\n",
    "    ('decay_Q', 'λ_Q (Q memory)', 'Query state persistence'),\n",
    "    ('decay_K', 'λ_K (K memory)', 'Key state persistence'),\n",
    "    ('decay_A', 'λ_A (A memory)', 'Attention memory persistence'),\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TransformerCSSM MECHANISM ATTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Mechanism':<25} {'Description':<30} {'Pos':>10} {'Neg':>10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for key, name, desc in mechanisms:\n",
    "    if key in pos_param_grads['cssm_0']:\n",
    "        pos_mag = np.abs(np.array(pos_param_grads['cssm_0'][key])).mean()\n",
    "        neg_mag = np.abs(np.array(neg_param_grads['cssm_0'][key])).mean()\n",
    "        print(f\"{name:<25} {desc:<30} {pos_mag:>10.6f} {neg_mag:>10.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel gradient\n",
    "K_grad_pos = np.array(pos_param_grads['cssm_0']['kernel'])\n",
    "K_grad_neg = np.array(neg_param_grads['cssm_0']['kernel'])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "im0 = axes[0].imshow(K_grad_pos.mean(axis=0), cmap='RdBu_r')\n",
    "axes[0].set_title('Kernel grad (Positive)', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "plt.colorbar(im0, ax=axes[0], fraction=0.046)\n",
    "\n",
    "im1 = axes[1].imshow(K_grad_neg.mean(axis=0), cmap='RdBu_r')\n",
    "axes[1].set_title('Kernel grad (Negative)', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "plt.colorbar(im1, ax=axes[1], fraction=0.046)\n",
    "\n",
    "diff = K_grad_pos.mean(axis=0) - K_grad_neg.mean(axis=0)\n",
    "im2 = axes[2].imshow(diff, cmap='RdBu_r')\n",
    "axes[2].set_title('Difference (Pos - Neg)', fontsize=12)\n",
    "axes[2].axis('off')\n",
    "plt.colorbar(im2, ax=axes[2], fraction=0.046)\n",
    "\n",
    "plt.suptitle('Kernel Gradients: How Should K Change for Each Decision?', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# Part IV: Summary\n---\n\n## What We've Learned\n\n### The Problem\n- **Transformers** compute attention instantly but can't iterate or backtrack\n- **RNNs** can iterate but are sequential (slow on GPUs)\n- **Tasks like Pathfinder** require iterative, growing-receptive-field reasoning\n\n### The Solution: TransformerCSSM\n\n| Component | Inspiration | Benefit |\n|-----------|-------------|---------|\n| Q-K interaction | Transformers | Expressive attention-like computation |\n| Growing receptive field | hGRU | Local → global reasoning over time |\n| Associative scan | State Space Models | O(log T) parallel computation |\n| Attention accumulator (A) | Novel | Memory of Q-K correlations |\n\n### The Equations (Recap)\n\n$$Q_{t+1} = \\lambda_Q Q_t + w (\\mathcal{K} * K_t) + \\alpha (\\mathcal{K} * A_t) + U_Q$$\n$$K_{t+1} = w (\\mathcal{K} * Q_t) + \\lambda_K K_t + U_K$$\n$$A_{t+1} = \\gamma (Q_t + K_t) + \\lambda_A A_t + U_A$$\n\n### Results\n\n| Model | Architecture | Pathfinder-14 Accuracy |\n|-------|--------------|------------------------|\n| Standard Transformer | 12 layers, attention | ~75% |\n| hGRU (sequential RNN) | Bilinear E-I dynamics | ~90% |\n| **TransformerCSSM** | Q-K-A with parallel scan | **88.50%** |\n\nTransformerCSSM achieves near-hGRU performance while being **parallelizable**—the best of both worlds."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*70)\nprint(\"KEY TAKEAWAYS\")\nprint(\"=\"*70)\nprint()\nprint(\"1. CSSM = RNN that runs in parallel via associative scan\")\nprint()\nprint(\"2. Basic CSSM is limited (single state variable)\")\nprint()\nprint(\"3. TransformerCSSM adds Q-K-A dynamics inspired by:\")\nprint(\"   • Transformer attention (Q-K interaction)\")\nprint(\"   • hGRU (bilinear terms, growing receptive fields)\")\nprint()\nprint(\"4. The model achieves 88.50% on Pathfinder-14\")\nprint(\"   (comparable to sequential hGRU, but parallelizable!)\")\nprint()\nprint(\"5. Key mechanisms:\")\nprint(\"   • Symmetric Q↔K coupling through spatial kernel\")\nprint(\"   • A accumulates Q-K history (attention memory)\")\nprint(\"   • A feeds back into Q (attention modulation)\")\nprint()\nprint(\"=\"*70)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}