{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# TransformerCSSM Interpretability\n\nThis notebook analyzes the **TransformerCSSM** - a simplified, transformer-inspired variant of CSSM that makes the attention analogy explicit.\n\n**Model Performance:** 88.50% accuracy on Pathfinder-14 (64-dim model)\n\n**Contents:**\n1. Mathematical foundations - Q/K/A dynamics\n2. Comparison with HGRUBilinearCSSM\n3. Sequential vs Parallel execution\n4. Temporal gradient attribution\n5. Mechanism attribution\n\n**Key Insight:** TransformerCSSM makes the attention mechanism explicit:\n- **Q (Query)** ≈ current representation seeking context\n- **K (Key)** ≈ stored information to match against\n- **A (Attention)** ≈ accumulated Q-K correlation that feeds back into Q\n\n---\n## Setup (Run First)"
  },
  {
   "cell_type": "code",
   "source": "#@title Setup: Install dependencies and download checkpoint { display-mode: \"form\" }\nimport os\nimport sys\n\n# Check if running in Colab\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    # Install JAX with GPU support\n    !pip install -q jax[cuda12] flax optax tensorflow\n    \n    # Clone CSSM repo\n    if not os.path.exists('CSSM'):\n        !git clone https://github.com/your-repo/CSSM.git\n        sys.path.insert(0, 'CSSM')\n    \n    # Download checkpoint (--no-check-certificate for expired SSL cert)\n    CHECKPOINT_URL = \"https://connectomics.clps.brown.edu/tf_records/transformer_cssm_kqv64_epoch20.pkl\"\n    CHECKPOINT_PATH = \"transformer_cssm_checkpoint.pkl\"\n    \n    if not os.path.exists(CHECKPOINT_PATH):\n        print(f\"Downloading checkpoint from {CHECKPOINT_URL}...\")\n        !wget -q --no-check-certificate {CHECKPOINT_URL} -O {CHECKPOINT_PATH}\n        print(\"Download complete!\")\nelse:\n    # Local paths\n    CHECKPOINT_PATH = \"checkpoints/KQV_64/epoch_20/checkpoint.pkl\"\n\nprint(f\"Checkpoint path: {CHECKPOINT_PATH}\")\nprint(f\"Running in {'Colab' if IN_COLAB else 'local'} environment\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part I: Mathematical Foundations\n",
    "---\n",
    "\n",
    "## 1. The TransformerCSSM Equations\n",
    "\n",
    "### 1.1 State Variables\n",
    "\n",
    "TransformerCSSM has three states with explicit transformer naming:\n",
    "\n",
    "$$\\mathbf{s}_t = (Q_t, K_t, A_t) \\in \\mathbb{R}^{H \\times W \\times C}$$\n",
    "\n",
    "| State | Name | Role | Transformer Analogy |\n",
    "|-------|------|------|--------------------|\n",
    "| $Q_t$ | Query | Current representation | Queries seeking relevant info |\n",
    "| $K_t$ | Key | Symmetric partner to Q | Keys to match against |\n",
    "| $A_t$ | Attention | Accumulated Q-K history | Attention weights over time |\n",
    "\n",
    "### 1.2 Dynamics\n",
    "\n",
    "$$\\boxed{Q_{t+1} = \\underbrace{\\lambda_Q \\cdot Q_t}_{\\text{decay}} + \\underbrace{w \\cdot (\\mathcal{K} * K_t)}_{\\text{K input via kernel}} + \\underbrace{\\alpha \\cdot (\\mathcal{K} * A_t)}_{\\text{attention feedback}} + U_Q}$$\n",
    "\n",
    "$$\\boxed{K_{t+1} = \\underbrace{w \\cdot (\\mathcal{K} * Q_t)}_{\\text{Q input via kernel}} + \\underbrace{\\lambda_K \\cdot K_t}_{\\text{decay}} + U_K}$$\n",
    "\n",
    "$$\\boxed{A_{t+1} = \\underbrace{\\gamma \\cdot Q_t}_{\\text{Q accumulation}} + \\underbrace{\\gamma \\cdot K_t}_{\\text{K accumulation}} + \\underbrace{\\lambda_A \\cdot A_t}_{\\text{decay}} + U_A}$$\n",
    "\n",
    "where:\n",
    "- $\\mathcal{K}$ is a **single learned spatial kernel** (not two like HGRUBilinearCSSM)\n",
    "- $*$ denotes spatial convolution (via FFT)\n",
    "- $w$ is the **Q↔K coupling weight** (symmetric!)\n",
    "- $\\alpha$ is the **attention feedback strength**\n",
    "- $\\gamma$ is the **attention accumulation rate**\n",
    "- $\\lambda_Q, \\lambda_K, \\lambda_A \\in (0.1, 0.99)$ are decay rates\n",
    "\n",
    "### 1.3 Matrix Form\n",
    "\n",
    "$$\\begin{bmatrix} Q \\\\ K \\\\ A \\end{bmatrix}_{t+1} = \n",
    "\\begin{bmatrix} \n",
    "\\lambda_Q & w \\cdot \\mathcal{K} & \\alpha \\cdot \\mathcal{K} \\\\\n",
    "w \\cdot \\mathcal{K} & \\lambda_K & 0 \\\\\n",
    "\\gamma & \\gamma & \\lambda_A\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} Q \\\\ K \\\\ A \\end{bmatrix}_t + \n",
    "\\begin{bmatrix} U_Q \\\\ U_K \\\\ U_A \\end{bmatrix}$$\n",
    "\n",
    "**Key observations:**\n",
    "1. **Symmetric Q↔K coupling**: Both use $w \\cdot \\mathcal{K}$ (same weight)\n",
    "2. **A is pure memory**: No kernel in A's row - just scalar accumulation\n",
    "3. **A feeds back to Q only**: The $\\alpha \\cdot \\mathcal{K} \\cdot A$ term\n",
    "\n",
    "### 1.4 The Attention Interpretation\n",
    "\n",
    "Think of the dynamics as an **iterative attention mechanism**:\n",
    "\n",
    "1. **Q and K interact symmetrically** via the spatial kernel $\\mathcal{K}$:\n",
    "   - Q receives K through the kernel (sees K's spatial context)\n",
    "   - K receives Q through the kernel (symmetric)\n",
    "   \n",
    "2. **A accumulates Q-K correlation**:\n",
    "   - $A \\leftarrow \\gamma (Q + K) + \\lambda_A A$\n",
    "   - This is like building up attention weights over time\n",
    "   \n",
    "3. **A feeds back into Q**:\n",
    "   - $Q \\leftarrow ... + \\alpha \\cdot \\mathcal{K} * A$\n",
    "   - The accumulated attention modulates Q (like applying attention weights)\n",
    "\n",
    "**As receptive fields grow** (kernel compounds over timesteps), A accumulates Q-K correlations across increasingly large spatial regions - achieving attention-like global context integration with O(T) or O(log T) complexity instead of O(T²)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the equations\n",
    "print(\"=\"*75)\n",
    "print(\"TransformerCSSM STATE UPDATE EQUATIONS\")\n",
    "print(\"=\"*75)\n",
    "print()\n",
    "print(\"Q_{t+1} = λ_Q·Q_t + w·(K * K_t) + α·(K * A_t) + U_Q\")\n",
    "print(\"         ────────   ───────────   ───────────\")\n",
    "print(\"         decay      K input       attention feedback\")\n",
    "print(\"                    (via kernel)  (A modulates Q!)\")\n",
    "print()\n",
    "print(\"K_{t+1} = w·(K * Q_t) + λ_K·K_t + U_K\")\n",
    "print(\"         ───────────   ────────\")\n",
    "print(\"         Q input       decay\")\n",
    "print(\"         (symmetric!)\")\n",
    "print()\n",
    "print(\"A_{t+1} = γ·Q_t + γ·K_t + λ_A·A_t + U_A\")\n",
    "print(\"         ──────   ──────   ────────\")\n",
    "print(\"         accumulate Q+K   decay (pure memory, NO kernel)\")\n",
    "print()\n",
    "print(\"=\"*75)\n",
    "print(\"K = single spatial kernel (applied via FFT convolution)\")\n",
    "print(\"w = Q↔K coupling weight (same for both directions)\")\n",
    "print(\"α = attention feedback strength (A → Q)\")\n",
    "print(\"γ = attention accumulation rate (Q,K → A)\")\n",
    "print(\"=\"*75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Comparison: TransformerCSSM vs HGRUBilinearCSSM\n\n| Aspect | HGRUBilinearCSSM | TransformerCSSM |\n|--------|------------------|------------------|\n| **States** | X (excit), Y (inhib), Z (interact) | Q (query), K (key), A (attention) |\n| **Kernels** | 2: K_E (excit), K_I (inhib) | 1: K (shared) |\n| **Coupling** | Asymmetric (α_E, α_I, μ_E, μ_I) | Symmetric (single w) |\n| **Bilinear term** | X * Z (multiplicative) | α·K·A (gated feedback) |\n| **Memory state** | Z has kernel: δ·(X-Y) | A is pure memory: γ·(Q+K) |\n| **Gates** | ~13 | ~10 |\n| **Accuracy (PF14)** | 85.50% (32-dim) | **88.50%** (64-dim) |\n\n### Key Simplifications\n\n1. **Single kernel** instead of E/I pair → Simpler, but loses explicit E-I dynamics\n2. **Symmetric coupling** → Cleaner Q↔K relationship\n3. **Pure memory A** → No spatial kernel on A, just accumulates Q+K\n4. **Linear A feedback** → $\\alpha \\cdot K * A$ instead of $\\gamma \\cdot X \\odot Z$ (bilinear)\n\nThe TransformerCSSM with 64 dimensions **outperforms** the 32-dim HGRUBilinearCSSM, suggesting the simpler attention-like structure scales well with increased capacity."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Matrix comparison:\")\n",
    "print()\n",
    "print(\"HGRUBilinearCSSM:                    TransformerCSSM:\")\n",
    "print(\"┌                              ┐    ┌                        ┐\")\n",
    "print(\"│ λ_x + α_E·K_E + γ·Z  -α_I·K_I  0 │    │ λ_Q      w·K      α·K │\")\n",
    "print(\"│ μ_E            λ_y - μ_I   0 │    │ w·K      λ_K       0   │\")\n",
    "print(\"│ δ                  -δ     λ_z │    │ γ        γ        λ_A │\")\n",
    "print(\"└                              ┘    └                        ┘\")\n",
    "print()\n",
    "print(\"Key differences:\")\n",
    "print(\"  • HGRUBi has γ·Z in diagonal (state-dependent, bilinear)\")\n",
    "print(\"  • Transformer has single kernel K (simpler)\")\n",
    "print(\"  • Transformer has symmetric w (same for Q→K and K→Q)\")\n",
    "print(\"  • Transformer A row has NO kernel (pure scalar memory)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part II: Sequential vs Parallel Execution\n",
    "---\n",
    "\n",
    "## 3. Implementation Comparison\n",
    "\n",
    "TransformerCSSM uses the **same associative scan** as HGRUBilinearCSSM, but with a cleaner matrix structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "def spectral_conv_2d(x: jnp.ndarray, kernel: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"FFT-based 2D convolution.\"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    k = kernel.shape[1]\n",
    "    \n",
    "    pad_h, pad_w = (H - k) // 2, (W - k) // 2\n",
    "    kernel_padded = jnp.pad(kernel, ((0, 0), (pad_h, H - k - pad_h), (pad_w, W - k - pad_w)))\n",
    "    \n",
    "    X_fft = jnp.fft.rfft2(x, axes=(1, 2))\n",
    "    K_fft = jnp.fft.rfft2(kernel_padded, axes=(1, 2))\n",
    "    K_fft = jnp.moveaxis(K_fft, 0, -1)\n",
    "    \n",
    "    result_fft = X_fft * K_fft[None, ...]\n",
    "    return jnp.fft.irfft2(result_fft, s=(H, W), axes=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_cssm_step_sequential(\n",
    "    Q: jnp.ndarray, K_state: jnp.ndarray, A: jnp.ndarray,\n",
    "    U_Q: jnp.ndarray, U_K: jnp.ndarray, U_A: jnp.ndarray,\n",
    "    K_kernel: jnp.ndarray,\n",
    "    lambda_Q: float, lambda_K: float, lambda_A: float,\n",
    "    w: float, alpha: float, gamma: float,\n",
    ") -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"\n",
    "    Single TransformerCSSM timestep (SEQUENTIAL version for interpretability).\n",
    "    \n",
    "    Implements:\n",
    "        Q_{t+1} = λ_Q·Q + w·(K * K_state) + α·(K * A) + U_Q\n",
    "        K_{t+1} = w·(K * Q) + λ_K·K_state + U_K\n",
    "        A_{t+1} = γ·Q + γ·K_state + λ_A·A + U_A\n",
    "    \"\"\"\n",
    "    # Spatial convolutions through the SINGLE kernel\n",
    "    K_conv_Q = spectral_conv_2d(Q, K_kernel)        # K * Q\n",
    "    K_conv_K = spectral_conv_2d(K_state, K_kernel)  # K * K_state\n",
    "    K_conv_A = spectral_conv_2d(A, K_kernel)        # K * A\n",
    "    \n",
    "    # State updates\n",
    "    Q_new = lambda_Q * Q + w * K_conv_K + alpha * K_conv_A + U_Q\n",
    "    K_new = w * K_conv_Q + lambda_K * K_state + U_K\n",
    "    A_new = gamma * Q + gamma * K_state + lambda_A * A + U_A\n",
    "    \n",
    "    return Q_new, K_new, A_new\n",
    "\n",
    "\n",
    "def transformer_cssm_forward_sequential(\n",
    "    U: jnp.ndarray,\n",
    "    K_kernel: jnp.ndarray,\n",
    "    gates: Dict[str, float],\n",
    "    T: int = 8,\n",
    ") -> Tuple[list, list, list]:\n",
    "    \"\"\"\n",
    "    Full sequential forward pass.\n",
    "    \n",
    "    Complexity: O(T) - must process each timestep in sequence.\n",
    "    \"\"\"\n",
    "    B, H, W, C = U.shape\n",
    "    \n",
    "    # Initialize states to zero\n",
    "    Q = jnp.zeros((B, H, W, C))\n",
    "    K_state = jnp.zeros((B, H, W, C))\n",
    "    A = jnp.zeros((B, H, W, C))\n",
    "    \n",
    "    Q_history, K_history, A_history = [Q], [K_state], [A]\n",
    "    \n",
    "    for t in range(T):\n",
    "        Q, K_state, A = transformer_cssm_step_sequential(\n",
    "            Q, K_state, A, U, U, U, K_kernel,\n",
    "            gates['lambda_Q'], gates['lambda_K'], gates['lambda_A'],\n",
    "            gates['w'], gates['alpha'], gates['gamma'],\n",
    "        )\n",
    "        Q_history.append(Q)\n",
    "        K_history.append(K_state)\n",
    "        A_history.append(A)\n",
    "    \n",
    "    return Q_history, K_history, A_history\n",
    "\n",
    "print(\"Sequential implementation defined.\")\n",
    "print(\"  - transformer_cssm_step_sequential(): Single timestep\")\n",
    "print(\"  - transformer_cssm_forward_sequential(): O(T) forward pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*75)\n",
    "print(\"SEQUENTIAL vs PARALLEL COMPUTATION\")\n",
    "print(\"=\"*75)\n",
    "print()\n",
    "print(\"SEQUENTIAL (for loop):\")\n",
    "print(\"  for t in range(T):\")\n",
    "print(\"      Q[t+1] = λ_Q·Q[t] + w·(K * K[t]) + α·(K * A[t]) + U\")\n",
    "print(\"      K[t+1] = w·(K * Q[t]) + λ_K·K[t] + U\")\n",
    "print(\"      A[t+1] = γ·Q[t] + γ·K[t] + λ_A·A[t] + U\")\n",
    "print()\n",
    "print(\"  Time: O(T), Space: O(1)\")\n",
    "print()\n",
    "print(\"-\"*75)\n",
    "print()\n",
    "print(\"PARALLEL (associative scan):\")\n",
    "print(\"  # Rewrite as: s[t+1] = A_mat[t] · s[t] + b[t]\")\n",
    "print(\"  # where s = [Q, K, A]^T\")\n",
    "print()\n",
    "print(\"  A_mat = [[λ_Q,  w·K,  α·K],\")\n",
    "print(\"           [w·K,  λ_K,   0 ],\")\n",
    "print(\"           [ γ,    γ,   λ_A]]\")\n",
    "print()\n",
    "print(\"  # Use associative scan with composition:\")\n",
    "print(\"  # (A₂, b₂) ∘ (A₁, b₁) = (A₂·A₁, A₂·b₁ + b₂)\")\n",
    "print()\n",
    "print(\"  Time: O(log T), Space: O(T)\")\n",
    "print()\n",
    "print(\"=\"*75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part III: Hands-On Analysis\n",
    "---\n",
    "\n",
    "## 4. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pickle\nimport matplotlib.pyplot as plt\n\n# Load checkpoint (uses CHECKPOINT_PATH from setup cell)\nwith open(CHECKPOINT_PATH, 'rb') as f:\n    ckpt = pickle.load(f)\n\nparams = ckpt['params']\nprint(f\"Loaded checkpoint from epoch {ckpt.get('epoch', 'unknown')}\")\nprint(f\"CSSM params: {list(params['cssm_0'].keys())}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract TransformerCSSM parameters\ncssm_params = params['cssm_0']\n\n# Single spatial kernel (NOT two like HGRUBi)\nK_kernel = jnp.array(cssm_params['kernel'])  # (C, k, k)\nprint(f\"Spatial kernel shape: {K_kernel.shape}\")\n\n# Extract gate values - handle Dense layer dict structure\ndef get_gate(name):\n    \"\"\"Extract gate value, handling Dense layer dict format.\"\"\"\n    gate_data = cssm_params[name]\n    # TransformerCSSM stores gates as Dense layers: {'kernel': ..., 'bias': ...}\n    if isinstance(gate_data, dict):\n        kernel = gate_data['kernel']\n        bias = gate_data.get('bias', 0)\n        val = kernel.mean() + (bias.mean() if hasattr(bias, 'mean') else bias)\n    else:\n        val = gate_data.mean()\n    return float(jax.nn.sigmoid(val))\n\ngates = {\n    'lambda_Q': get_gate('decay_Q'),\n    'lambda_K': get_gate('decay_K'),\n    'lambda_A': get_gate('decay_A'),\n    'w': get_gate('w_qk'),\n    'alpha': get_gate('alpha'),\n    'gamma': get_gate('gamma'),\n}\n\nprint(\"\\nExtracted gate values:\")\nfor name, val in gates.items():\n    print(f\"  {name:>10}: {val:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize the single kernel (64 channels for KQV_64 model)\nfig, axes = plt.subplots(4, 8, figsize=(16, 8))\n\nK_np = np.array(K_kernel)\nvmax = np.abs(K_np).max()\n\nfor row in range(4):\n    for col in range(8):\n        idx = row * 8 + col\n        if idx < K_np.shape[0]:\n            axes[row, col].imshow(K_np[idx], cmap='RdBu_r', vmin=-vmax, vmax=vmax)\n            axes[row, col].set_title(f'K[{idx}]', fontsize=8)\n        axes[row, col].axis('off')\n\nplt.suptitle('TransformerCSSM: Single Spatial Kernel (64 channels, shared for Q↔K and A→Q)', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n# Mean kernel\nfig, ax = plt.subplots(figsize=(5, 4))\nim = ax.imshow(K_np.mean(axis=0), cmap='RdBu_r')\nax.set_title('Mean Kernel (averaged over 64 channels)', fontsize=12)\nax.axis('off')\nplt.colorbar(im, ax=ax, fraction=0.046)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load model for gradient computation\nfrom src.models.simple_cssm import SimpleCSSM\n\nmodel = SimpleCSSM(\n    num_classes=2,\n    embed_dim=64,  # 64-dim model (KQV_64)\n    depth=1,\n    cssm_type='transformer',  # TransformerCSSM!\n    kernel_size=15,\n    pos_embed='spatiotemporal',\n    seq_len=8,\n)\nprint(\"TransformerCSSM model loaded (embed_dim=64).\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample images\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "TFRECORD_DIR = '/home/dlinsley/pathfinder_tfrecord/difficulty_14/val'\n",
    "\n",
    "def parse_example(example):\n",
    "    features = tf.io.parse_single_example(example, {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    })\n",
    "    image = tf.io.decode_raw(features['image'], tf.float32)\n",
    "    image = tf.reshape(image, [224, 224, 3])\n",
    "    return image, features['label']\n",
    "\n",
    "val_files = sorted(tf.io.gfile.glob(f'{TFRECORD_DIR}/*.tfrecord'))\n",
    "ds = tf.data.TFRecordDataset(val_files[:1]).map(parse_example)\n",
    "\n",
    "pos_img, neg_img = None, None\n",
    "for img, label in ds:\n",
    "    if label.numpy() == 1 and pos_img is None:\n",
    "        pos_img = img.numpy()\n",
    "    elif label.numpy() == 0 and neg_img is None:\n",
    "        neg_img = img.numpy()\n",
    "    if pos_img is not None and neg_img is not None:\n",
    "        break\n",
    "\n",
    "print(f\"Loaded positive image: {pos_img.shape}\")\n",
    "print(f\"Loaded negative image: {neg_img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify predictions\n",
    "def forward_single(img):\n",
    "    x = jnp.array(img)[None, ...]\n",
    "    x_temporal = jnp.repeat(x[:, None, ...], 8, axis=1)\n",
    "    logits = model.apply({'params': params}, x_temporal, training=False)\n",
    "    return logits[0]\n",
    "\n",
    "pos_logits = forward_single(pos_img)\n",
    "neg_logits = forward_single(neg_img)\n",
    "\n",
    "print(\"TransformerCSSM Predictions:\")\n",
    "print(f\"  Positive: {pos_logits} → {'Connected' if pos_logits.argmax() == 1 else 'Disconnected'}\")\n",
    "print(f\"  Negative: {neg_logits} → {'Connected' if neg_logits.argmax() == 1 else 'Disconnected'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Temporal Gradient Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_temporal_gradients(img, target_class):\n",
    "    x = jnp.array(img)[None, ...]\n",
    "    x_temporal = jnp.repeat(x[:, None, ...], 8, axis=1)\n",
    "    \n",
    "    def forward_fn(x_t):\n",
    "        logits = model.apply({'params': params}, x_t, training=False)\n",
    "        return logits[0, target_class]\n",
    "    \n",
    "    grads = jax.grad(forward_fn)(x_temporal)\n",
    "    grad_magnitude = jnp.abs(grads).sum(axis=(0, 2, 3, 4))\n",
    "    spatial_grads = jnp.abs(grads[0]).sum(axis=-1)\n",
    "    return grad_magnitude, spatial_grads\n",
    "\n",
    "pos_grad_mag, pos_spatial = compute_temporal_gradients(pos_img, 1)\n",
    "neg_grad_mag, neg_spatial = compute_temporal_gradients(neg_img, 0)\n",
    "\n",
    "print(\"Gradient magnitude per timestep:\")\n",
    "print(f\"  Positive: {np.array(pos_grad_mag).round(2)}\")\n",
    "print(f\"  Negative: {np.array(neg_grad_mag).round(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot temporal importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "timesteps = np.arange(8)\n",
    "width = 0.35\n",
    "axes[0].bar(timesteps - width/2, np.array(pos_grad_mag), width, \n",
    "           label='Positive', color='green', alpha=0.7)\n",
    "axes[0].bar(timesteps + width/2, np.array(neg_grad_mag), width,\n",
    "           label='Negative', color='red', alpha=0.7)\n",
    "axes[0].set_xlabel('Timestep', fontsize=12)\n",
    "axes[0].set_ylabel('Gradient Magnitude', fontsize=12)\n",
    "axes[0].set_title('TransformerCSSM: Temporal Importance', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].set_xticks(timesteps)\n",
    "\n",
    "# Cumulative\n",
    "pos_cumsum = np.cumsum(np.array(pos_grad_mag))\n",
    "neg_cumsum = np.cumsum(np.array(neg_grad_mag))\n",
    "axes[1].plot(timesteps, pos_cumsum / pos_cumsum[-1], 'g-o', label='Positive', linewidth=2)\n",
    "axes[1].plot(timesteps, neg_cumsum / neg_cumsum[-1], 'r-o', label='Negative', linewidth=2)\n",
    "axes[1].set_xlabel('Timestep', fontsize=12)\n",
    "axes[1].set_ylabel('Cumulative Importance', fontsize=12)\n",
    "axes[1].set_title('Information Integration Over Time', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].set_xticks(timesteps)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mechanism Attribution: Which Gates Matter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_gradients(img, target_class):\n",
    "    x = jnp.array(img)[None, ...]\n",
    "    x_temporal = jnp.repeat(x[:, None, ...], 8, axis=1)\n",
    "    \n",
    "    def loss_fn(p):\n",
    "        logits = model.apply({'params': p}, x_temporal, training=False)\n",
    "        return logits[0, target_class]\n",
    "    \n",
    "    return jax.grad(loss_fn)(params)\n",
    "\n",
    "pos_param_grads = parameter_gradients(pos_img, 1)\n",
    "neg_param_grads = parameter_gradients(neg_img, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransformerCSSM-specific gates\n",
    "mechanisms = [\n",
    "    ('w_qk', 'w (Q↔K coupling)', 'Symmetric Q-K interaction'),\n",
    "    ('alpha', 'α (A→Q feedback)', 'Attention feeds back to Q'),\n",
    "    ('gamma', 'γ (Q,K→A accum)', 'Attention accumulation rate'),\n",
    "    ('decay_Q', 'λ_Q (Q memory)', 'Query state persistence'),\n",
    "    ('decay_K', 'λ_K (K memory)', 'Key state persistence'),\n",
    "    ('decay_A', 'λ_A (A memory)', 'Attention memory persistence'),\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TransformerCSSM MECHANISM ATTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Mechanism':<25} {'Description':<30} {'Pos':>10} {'Neg':>10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for key, name, desc in mechanisms:\n",
    "    if key in pos_param_grads['cssm_0']:\n",
    "        pos_mag = np.abs(np.array(pos_param_grads['cssm_0'][key])).mean()\n",
    "        neg_mag = np.abs(np.array(neg_param_grads['cssm_0'][key])).mean()\n",
    "        print(f\"{name:<25} {desc:<30} {pos_mag:>10.6f} {neg_mag:>10.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel gradient\n",
    "K_grad_pos = np.array(pos_param_grads['cssm_0']['kernel'])\n",
    "K_grad_neg = np.array(neg_param_grads['cssm_0']['kernel'])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "im0 = axes[0].imshow(K_grad_pos.mean(axis=0), cmap='RdBu_r')\n",
    "axes[0].set_title('Kernel grad (Positive)', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "plt.colorbar(im0, ax=axes[0], fraction=0.046)\n",
    "\n",
    "im1 = axes[1].imshow(K_grad_neg.mean(axis=0), cmap='RdBu_r')\n",
    "axes[1].set_title('Kernel grad (Negative)', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "plt.colorbar(im1, ax=axes[1], fraction=0.046)\n",
    "\n",
    "diff = K_grad_pos.mean(axis=0) - K_grad_neg.mean(axis=0)\n",
    "im2 = axes[2].imshow(diff, cmap='RdBu_r')\n",
    "axes[2].set_title('Difference (Pos - Neg)', fontsize=12)\n",
    "axes[2].axis('off')\n",
    "plt.colorbar(im2, ax=axes[2], fraction=0.046)\n",
    "\n",
    "plt.suptitle('Kernel Gradients: How Should K Change for Each Decision?', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# Part IV: Summary\n---\n\n## Key Findings\n\n### 1. TransformerCSSM Equations\n\n$$Q_{t+1} = \\lambda_Q Q_t + w (\\mathcal{K} * K_t) + \\alpha (\\mathcal{K} * A_t) + U_Q$$\n$$K_{t+1} = w (\\mathcal{K} * Q_t) + \\lambda_K K_t + U_K$$\n$$A_{t+1} = \\gamma Q_t + \\gamma K_t + \\lambda_A A_t + U_A$$\n\n### 2. Comparison with HGRUBilinearCSSM\n\n| | HGRUBilinearCSSM (32-dim) | TransformerCSSM (64-dim) |\n|--|---------------------------|--------------------------|\n| States | X, Y, Z | Q, K, A |\n| Kernels | 2 (K_E, K_I) | 1 (shared) |\n| Coupling | Asymmetric | Symmetric |\n| Gates | ~13 | ~10 |\n| Accuracy | 85.50% | **88.50%** |\n\n### 3. The Attention Mechanism\n\nTransformerCSSM makes the attention analogy explicit:\n- **Q** seeks relevant context (like queries)\n- **K** provides context to match (like keys)\n- **A** accumulates Q-K correlation (like attention weights)\n- **A feeds back to Q** (like applying attention)\n\n### 4. Why It Works\n\nDespite being simpler than HGRUBilinearCSSM, TransformerCSSM achieves **better** accuracy (88.50% vs 85.50%) because:\n1. The **single kernel** is sufficient for contour spreading\n2. **Symmetric Q↔K** captures the essential bidirectional information flow\n3. **A as pure memory** (no kernel) still accumulates the necessary context\n4. The **attention feedback** (α·K·A → Q) provides the key modulation\n5. **64-dim embedding** provides more representational capacity"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*70)\nprint(\"NOTEBOOK COMPLETE\")\nprint(\"=\"*70)\nprint()\nprint(\"TransformerCSSM Key Insights:\")\nprint(\"  1. Achieves 88.50% accuracy (beats HGRUBilinearCSSM's 85.50%!)\")\nprint(\"  2. Uses explicit Q/K/A naming (transformer-like)\")\nprint(\"  3. Single kernel instead of E/I pair\")\nprint(\"  4. Symmetric Q↔K coupling via shared weight w\")\nprint(\"  5. A is pure memory that feeds back into Q (attention mechanism)\")\nprint(\"  6. 64-dim embedding provides more capacity than 32-dim\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}