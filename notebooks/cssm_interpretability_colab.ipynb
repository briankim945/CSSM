{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# CSSM Interpretability: Understanding Temporal Dynamics\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_REPO/blob/main/notebooks/cssm_interpretability_colab.ipynb)\n",
    "\n",
    "This notebook provides a deep dive into how CSSM (Cepstral State Space Models) processes visual information over time on the Pathfinder contour integration task.\n",
    "\n",
    "**Contents:**\n",
    "1. Setup and installation\n",
    "2. Load trained model (85.5% accuracy on PF14)\n",
    "3. **Mathematical foundations** - Full equation derivations\n",
    "4. **Sequential vs Parallel execution** - Compare RNN loop vs associative scan\n",
    "5. Temporal gradient attribution\n",
    "6. Step-by-step forward pass visualization\n",
    "7. Backward attribution to specific mechanisms\n",
    "\n",
    "**Key Insight:** CSSM uses temporal recurrence to iteratively grow receptive fields. The X*Z bilinear term acts similarly to iteratively growing attention, where Z tracks accumulated X-Y interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup\n",
    "\n",
    "Install dependencies and download model code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q jax jaxlib flax optax matplotlib\n",
    "\n",
    "# Clone repository (for model code)\n",
    "# !git clone https://github.com/YOUR_USERNAME/CSSM.git\n",
    "# %cd CSSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, lax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import pickle\n",
    "from typing import Dict, Tuple, NamedTuple, Callable\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "math_foundations"
   },
   "source": [
    "---\n",
    "# Part I: Mathematical Foundations\n",
    "---\n",
    "\n",
    "## 2. The CSSM Equations\n",
    "\n",
    "### 2.1 State Space Model Formulation\n",
    "\n",
    "CSSM is a **discrete-time state space model** with three coupled states:\n",
    "\n",
    "$$\\mathbf{s}_t = (X_t, Y_t, Z_t) \\in \\mathbb{R}^{H \\times W \\times C}$$\n",
    "\n",
    "where:\n",
    "- $X_t$: **Excitatory state** - main representation\n",
    "- $Y_t$: **Inhibitory state** - provides suppression\n",
    "- $Z_t$: **Interaction state** - accumulates E-I history\n",
    "\n",
    "### 2.2 The HGRUBilinearCSSM Dynamics\n",
    "\n",
    "The state update equations are:\n",
    "\n",
    "$$\\boxed{X_{t+1} = \\underbrace{\\lambda_x \\cdot X_t}_{\\text{decay}} + \\underbrace{\\alpha_E \\cdot (K_E * X_t)}_{\\text{excitatory spread}} - \\underbrace{\\alpha_I \\cdot (K_I * Y_t)}_{\\text{inhibitory spread}} + \\underbrace{\\gamma \\cdot X_t \\odot Z_t}_{\\text{bilinear attention}} + U}$$\n",
    "\n",
    "$$\\boxed{Y_{t+1} = \\underbrace{\\lambda_y \\cdot Y_t}_{\\text{decay}} + \\underbrace{\\mu_E \\cdot X_t}_{\\text{excitation input}} - \\underbrace{\\mu_I \\cdot Y_t}_{\\text{self-inhibition}} + U}$$\n",
    "\n",
    "$$\\boxed{Z_{t+1} = \\underbrace{\\lambda_z \\cdot Z_t}_{\\text{decay}} + \\underbrace{\\delta \\cdot (X_t - Y_t)}_{\\text{E-I difference}} + U}$$\n",
    "\n",
    "where:\n",
    "- $*$ denotes **spatial convolution** (implemented via FFT for efficiency)\n",
    "- $\\odot$ denotes **element-wise (Hadamard) product**\n",
    "- $K_E, K_I \\in \\mathbb{R}^{C \\times k \\times k}$ are learned **spatial kernels**\n",
    "- $\\lambda_x, \\lambda_y, \\lambda_z \\in (0, 1)$ are **decay rates** (sigmoid-gated)\n",
    "- $\\alpha_E, \\alpha_I, \\mu_E, \\mu_I, \\gamma, \\delta$ are **gating parameters**\n",
    "- $U \\in \\mathbb{R}^{H \\times W \\times C}$ is the **input** (same at each timestep)\n",
    "\n",
    "### 2.3 Matrix Form (for Associative Scan)\n",
    "\n",
    "We can write the dynamics in matrix form. Let $\\mathbf{s}_t = [X_t, Y_t, Z_t]^T$ be the stacked state vector:\n",
    "\n",
    "$$\\mathbf{s}_{t+1} = \\mathbf{A}_t \\cdot \\mathbf{s}_t + \\mathbf{B} \\cdot U$$\n",
    "\n",
    "The transition matrix $\\mathbf{A}_t$ is a **3×3 block matrix**:\n",
    "\n",
    "$$\\mathbf{A}_t = \\begin{bmatrix} \n",
    "\\lambda_x + \\alpha_E K_E + \\gamma Z_t & -\\alpha_I K_I & 0 \\\\\n",
    "\\mu_E & \\lambda_y - \\mu_I & 0 \\\\\n",
    "\\delta & -\\delta & \\lambda_z\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**Key insight**: The matrix is **state-dependent** due to the $\\gamma Z_t$ term in the (1,1) position. This bilinear term is what makes CSSM more expressive than a linear SSM.\n",
    "\n",
    "### 2.4 The Bilinear Term: Attention Analogy\n",
    "\n",
    "The term $\\gamma \\cdot X_t \\odot Z_t$ is crucial. Let's understand why:\n",
    "\n",
    "1. **Z accumulates history**: From $Z_{t+1} = \\lambda_z Z_t + \\delta(X_t - Y_t)$, we see Z is a decayed sum of past E-I differences:\n",
    "   $$Z_t = \\sum_{\\tau=0}^{t-1} \\lambda_z^{t-1-\\tau} \\cdot \\delta \\cdot (X_\\tau - Y_\\tau)$$\n",
    "\n",
    "2. **X*Z modulates based on history**: When we compute $X_t \\odot Z_t$, we're multiplying current activity by accumulated context.\n",
    "\n",
    "3. **Analogy to attention**:\n",
    "   - In transformers: $\\text{Attention} = \\text{softmax}(QK^T/\\sqrt{d}) \\cdot V$\n",
    "   - In CSSM: $X \\odot Z \\approx Q \\odot \\text{accumulated}(K)$\n",
    "   \n",
    "   The key difference: CSSM accumulates over **time** with spatial convolutions growing the receptive field, while attention computes over **sequence positions** directly.\n",
    "\n",
    "### 2.5 Spectral Convolution\n",
    "\n",
    "The spatial convolutions $K * X$ are computed efficiently using FFT:\n",
    "\n",
    "$$K * X = \\mathcal{F}^{-1}\\left(\\mathcal{F}(K) \\odot \\mathcal{F}(X)\\right)$$\n",
    "\n",
    "This reduces complexity from $O(H \\cdot W \\cdot k^2)$ to $O(H \\cdot W \\cdot \\log(HW))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "equation_visual"
   },
   "outputs": [],
   "source": [
    "# Visualize the equations\n",
    "print(\"=\"*70)\n",
    "print(\"CSSM STATE UPDATE EQUATIONS\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"X_{t+1} = λ_x·X_t + α_E·(K_E * X_t) - α_I·(K_I * Y_t) + γ·(X_t ⊙ Z_t) + U\")\n",
    "print(\"         ────────   ───────────────   ───────────────   ────────────\")\n",
    "print(\"         decay      excitatory        inhibitory        bilinear\")\n",
    "print(\"                    spread            spread            attention\")\n",
    "print()\n",
    "print(\"Y_{t+1} = λ_y·Y_t + μ_E·X_t - μ_I·Y_t + U\")\n",
    "print(\"         ────────   ───────   ───────\")\n",
    "print(\"         decay      exc input self-inh\")\n",
    "print()\n",
    "print(\"Z_{t+1} = λ_z·Z_t + δ·(X_t - Y_t) + U\")\n",
    "print(\"         ────────   ─────────────\")\n",
    "print(\"         decay      E-I difference\")\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"where * = spatial convolution, ⊙ = element-wise product\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "implementations"
   },
   "source": [
    "---\n",
    "# Part II: Sequential vs Parallel Implementation\n",
    "---\n",
    "\n",
    "## 3. Two Ways to Compute CSSM\n",
    "\n",
    "### 3.1 Sequential (RNN-style): O(T) complexity\n",
    "\n",
    "The straightforward approach: loop over timesteps.\n",
    "\n",
    "```python\n",
    "for t in range(T):\n",
    "    s[t+1] = f(s[t], u[t])\n",
    "```\n",
    "\n",
    "### 3.2 Parallel (Associative Scan): O(log T) complexity\n",
    "\n",
    "For **linear** recurrences $s_{t+1} = A_t \\cdot s_t + b_t$, we can use the **associative scan** algorithm.\n",
    "\n",
    "**Key insight**: The composition of two linear transforms is also linear:\n",
    "$$(A_2, b_2) \\circ (A_1, b_1) = (A_2 \\cdot A_1, A_2 \\cdot b_1 + b_2)$$\n",
    "\n",
    "This operation is **associative**, so we can compute it in parallel using a tree reduction:\n",
    "\n",
    "```\n",
    "Time 0:  (A₀,b₀)  (A₁,b₁)  (A₂,b₂)  (A₃,b₃)  (A₄,b₄)  (A₅,b₅)  (A₆,b₆)  (A₇,b₇)\n",
    "              \\    /            \\    /            \\    /            \\    /\n",
    "Time 1:      (A₀₁,b₀₁)        (A₂₃,b₂₃)        (A₄₅,b₄₅)        (A₆₇,b₆₇)\n",
    "                   \\          /                       \\          /\n",
    "Time 2:           (A₀₁₂₃,b₀₁₂₃)                     (A₄₅₆₇,b₄₅₆₇)\n",
    "                            \\                      /\n",
    "Time 3:                    (A₀₁₂₃₄₅₆₇,b₀₁₂₃₄₅₆₇)\n",
    "```\n",
    "\n",
    "This gives **O(log T)** parallel time complexity!\n",
    "\n",
    "**Challenge for CSSM**: The bilinear term $X \\odot Z$ makes the recurrence **non-linear**. We handle this by:\n",
    "1. Treating $Z_t$ as a time-varying parameter in the linear part\n",
    "2. Computing the scan in a modified state space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sequential_impl"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SEQUENTIAL IMPLEMENTATION (for interpretability)\n",
    "# ============================================================\n",
    "\n",
    "def spectral_conv_2d(x: jnp.ndarray, kernel: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    FFT-based 2D convolution: K * X = F^{-1}(F(K) ⊙ F(X))\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor (B, H, W, C)\n",
    "        kernel: Convolution kernel (C, k, k)\n",
    "    Returns:\n",
    "        Convolved output (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    k = kernel.shape[1]\n",
    "    \n",
    "    # Pad kernel to match input size (center it)\n",
    "    pad_h, pad_w = (H - k) // 2, (W - k) // 2\n",
    "    kernel_padded = jnp.pad(\n",
    "        kernel,\n",
    "        ((0, 0), (pad_h, H - k - pad_h), (pad_w, W - k - pad_w))\n",
    "    )  # (C, H, W)\n",
    "    \n",
    "    # FFT of input and kernel\n",
    "    X_fft = jnp.fft.rfft2(x, axes=(1, 2))  # (B, H, W_freq, C)\n",
    "    K_fft = jnp.fft.rfft2(kernel_padded, axes=(1, 2))  # (C, H, W_freq)\n",
    "    K_fft = jnp.moveaxis(K_fft, 0, -1)  # (H, W_freq, C)\n",
    "    \n",
    "    # Multiply in frequency domain\n",
    "    result_fft = X_fft * K_fft[None, ...]\n",
    "    \n",
    "    # Inverse FFT\n",
    "    return jnp.fft.irfft2(result_fft, s=(H, W), axes=(1, 2))\n",
    "\n",
    "\n",
    "def cssm_step_sequential(\n",
    "    X: jnp.ndarray, Y: jnp.ndarray, Z: jnp.ndarray,\n",
    "    U: jnp.ndarray,\n",
    "    K_E: jnp.ndarray, K_I: jnp.ndarray,\n",
    "    lambda_x: float, lambda_y: float, lambda_z: float,\n",
    "    alpha_E: float, alpha_I: float,\n",
    "    mu_E: float, mu_I: float,\n",
    "    gamma: float, delta: float,\n",
    ") -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"\n",
    "    Single CSSM timestep (sequential version).\n",
    "    \n",
    "    Implements:\n",
    "        X_{t+1} = λ_x·X + α_E·(K_E * X) - α_I·(K_I * Y) + γ·(X ⊙ Z) + U\n",
    "        Y_{t+1} = λ_y·Y + μ_E·X - μ_I·Y + U\n",
    "        Z_{t+1} = λ_z·Z + δ·(X - Y) + U\n",
    "    \"\"\"\n",
    "    # Spatial convolutions\n",
    "    K_E_X = spectral_conv_2d(X, K_E)  # Excitatory spread\n",
    "    K_I_Y = spectral_conv_2d(Y, K_I)  # Inhibitory spread\n",
    "    \n",
    "    # State updates\n",
    "    X_new = lambda_x * X + alpha_E * K_E_X - alpha_I * K_I_Y + gamma * X * Z + U\n",
    "    Y_new = lambda_y * Y + mu_E * X - mu_I * Y + U\n",
    "    Z_new = lambda_z * Z + delta * (X - Y) + U\n",
    "    \n",
    "    return X_new, Y_new, Z_new\n",
    "\n",
    "\n",
    "def cssm_forward_sequential(\n",
    "    U: jnp.ndarray,\n",
    "    K_E: jnp.ndarray, K_I: jnp.ndarray,\n",
    "    gates: Dict[str, float],\n",
    "    T: int = 8,\n",
    ") -> Tuple[list, list, list]:\n",
    "    \"\"\"\n",
    "    Full sequential forward pass.\n",
    "    \n",
    "    Returns history of all states for visualization.\n",
    "    \n",
    "    Complexity: O(T) - must process each timestep in sequence.\n",
    "    \"\"\"\n",
    "    B, H, W, C = U.shape\n",
    "    \n",
    "    # Initialize states to zero\n",
    "    X = jnp.zeros((B, H, W, C))\n",
    "    Y = jnp.zeros((B, H, W, C))\n",
    "    Z = jnp.zeros((B, H, W, C))\n",
    "    \n",
    "    # Store history\n",
    "    X_history, Y_history, Z_history = [X], [Y], [Z]\n",
    "    \n",
    "    # Sequential loop\n",
    "    for t in range(T):\n",
    "        X, Y, Z = cssm_step_sequential(\n",
    "            X, Y, Z, U, K_E, K_I,\n",
    "            gates['lambda_x'], gates['lambda_y'], gates['lambda_z'],\n",
    "            gates['alpha_E'], gates['alpha_I'],\n",
    "            gates['mu_E'], gates['mu_I'],\n",
    "            gates['gamma'], gates['delta'],\n",
    "        )\n",
    "        X_history.append(X)\n",
    "        Y_history.append(Y)\n",
    "        Z_history.append(Z)\n",
    "    \n",
    "    return X_history, Y_history, Z_history\n",
    "\n",
    "print(\"Sequential implementation defined.\")\n",
    "print(\"  - cssm_step_sequential(): Single timestep\")\n",
    "print(\"  - cssm_forward_sequential(): Full T-step forward pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "parallel_impl"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PARALLEL IMPLEMENTATION (Associative Scan)\n",
    "# ============================================================\n",
    "\n",
    "def associative_scan_op(elem1, elem2):\n",
    "    \"\"\"\n",
    "    Associative operator for linear recurrence composition.\n",
    "    \n",
    "    For linear recurrence s_{t+1} = A_t · s_t + b_t, we have:\n",
    "        (A₂, b₂) ∘ (A₁, b₁) = (A₂ · A₁, A₂ · b₁ + b₂)\n",
    "    \n",
    "    This is the key insight enabling parallel computation!\n",
    "    \n",
    "    Args:\n",
    "        elem1: (A₁, b₁) - first element\n",
    "        elem2: (A₂, b₂) - second element\n",
    "    Returns:\n",
    "        (A₂·A₁, A₂·b₁ + b₂) - composed element\n",
    "    \"\"\"\n",
    "    A1, b1 = elem1\n",
    "    A2, b2 = elem2\n",
    "    \n",
    "    # Matrix multiplication for A (element-wise for diagonal case)\n",
    "    A_new = A2 * A1  # Simplified: assuming diagonal A\n",
    "    b_new = A2 * b1 + b2\n",
    "    \n",
    "    return (A_new, b_new)\n",
    "\n",
    "\n",
    "def cssm_forward_parallel(\n",
    "    U: jnp.ndarray,\n",
    "    K_E: jnp.ndarray, K_I: jnp.ndarray,\n",
    "    gates: Dict[str, float],\n",
    "    T: int = 8,\n",
    ") -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"\n",
    "    Parallel forward pass using associative scan.\n",
    "    \n",
    "    For the linear parts of CSSM, we can use JAX's lax.associative_scan.\n",
    "    \n",
    "    Complexity: O(log T) parallel time (with O(T) work).\n",
    "    \n",
    "    Note: The bilinear X*Z term requires special handling.\n",
    "    Here we show a simplified version focusing on the linear dynamics.\n",
    "    \"\"\"\n",
    "    B, H, W, C = U.shape\n",
    "    \n",
    "    # For demonstration, we'll compute the Z state using associative scan\n",
    "    # Z_{t+1} = λ_z · Z_t + δ · (X_t - Y_t) + U\n",
    "    # This is a LINEAR recurrence: Z_{t+1} = A · Z_t + b_t\n",
    "    # where A = λ_z and b_t = δ·(X_t - Y_t) + U\n",
    "    \n",
    "    # First, we need X and Y at each timestep (computed sequentially for now)\n",
    "    # In a full implementation, all three would be computed together\n",
    "    \n",
    "    # Prepare elements for scan: (A, b) at each timestep\n",
    "    # A = λ_z (same at each step)\n",
    "    # b = δ·(X_t - Y_t) + U (varies with t)\n",
    "    \n",
    "    lambda_z = gates['lambda_z']\n",
    "    delta = gates['delta']\n",
    "    \n",
    "    # Create T copies of (A, b) - simplified: assume X=Y=0 for demo\n",
    "    # In practice, this would involve the full coupled system\n",
    "    As = jnp.full((T,), lambda_z)  # (T,)\n",
    "    bs = jnp.broadcast_to(U[None, ...], (T, B, H, W, C))  # (T, B, H, W, C)\n",
    "    \n",
    "    # Run associative scan\n",
    "    # This computes all prefix products in O(log T) parallel time!\n",
    "    def scan_fn(carry, x):\n",
    "        A_acc, b_acc = carry\n",
    "        A_new, b_new = x\n",
    "        return (A_acc * A_new, A_acc * b_new + b_acc), (A_acc * A_new, A_acc * b_new + b_acc)\n",
    "    \n",
    "    # Use lax.associative_scan for truly parallel execution\n",
    "    elements = (As, bs)\n",
    "    \n",
    "    # Associative scan computes prefix \"products\" under the associative op\n",
    "    _, (A_finals, Z_history) = lax.scan(\n",
    "        scan_fn, \n",
    "        (jnp.ones(()), jnp.zeros_like(U)),\n",
    "        elements\n",
    "    )\n",
    "    \n",
    "    return Z_history  # (T, B, H, W, C)\n",
    "\n",
    "\n",
    "print(\"Parallel implementation defined.\")\n",
    "print(\"  - associative_scan_op(): Composition operator (A₂,b₂)∘(A₁,b₁)\")\n",
    "print(\"  - cssm_forward_parallel(): O(log T) forward pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compare_implementations"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SIDE-BY-SIDE COMPARISON\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SEQUENTIAL vs PARALLEL CSSM COMPUTATION\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"┌─────────────────────────────────────────────────────────────────┐\")\n",
    "print(\"│ SEQUENTIAL (RNN Loop)                                           │\")\n",
    "print(\"├─────────────────────────────────────────────────────────────────┤\")\n",
    "print(\"│                                                                 │\")\n",
    "print(\"│   for t in range(T):                                           │\")\n",
    "print(\"│       X[t+1] = λ_x·X[t] + α_E·(K_E * X[t]) - α_I·(K_I * Y[t])  │\")\n",
    "print(\"│                + γ·(X[t] ⊙ Z[t]) + U                           │\")\n",
    "print(\"│       Y[t+1] = λ_y·Y[t] + μ_E·X[t] - μ_I·Y[t] + U              │\")\n",
    "print(\"│       Z[t+1] = λ_z·Z[t] + δ·(X[t] - Y[t]) + U                  │\")\n",
    "print(\"│                                                                 │\")\n",
    "print(\"│   Time Complexity: O(T)  - must wait for each step             │\")\n",
    "print(\"│   Space Complexity: O(1) - only need current state             │\")\n",
    "print(\"│                                                                 │\")\n",
    "print(\"│   Pros: Simple, intuitive, works with any nonlinearity         │\")\n",
    "print(\"│   Cons: Cannot parallelize across time                         │\")\n",
    "print(\"└─────────────────────────────────────────────────────────────────┘\")\n",
    "print()\n",
    "print(\"┌─────────────────────────────────────────────────────────────────┐\")\n",
    "print(\"│ PARALLEL (Associative Scan)                                     │\")\n",
    "print(\"├─────────────────────────────────────────────────────────────────┤\")\n",
    "print(\"│                                                                 │\")\n",
    "print(\"│   # Rewrite as: s[t+1] = A[t] · s[t] + b[t]                    │\")\n",
    "print(\"│   # Define composition: (A₂,b₂) ∘ (A₁,b₁) = (A₂·A₁, A₂·b₁+b₂) │\")\n",
    "print(\"│                                                                 │\")\n",
    "print(\"│   elements = [(A[0],b[0]), (A[1],b[1]), ..., (A[T-1],b[T-1])]  │\")\n",
    "print(\"│   prefixes = associative_scan(compose, elements)               │\")\n",
    "print(\"│   # prefixes[t] = (A[0:t], b[0:t]) = cumulative transform      │\")\n",
    "print(\"│   states = [prefix @ initial_state for prefix in prefixes]     │\")\n",
    "print(\"│                                                                 │\")\n",
    "print(\"│   Time Complexity: O(log T) - tree reduction                   │\")\n",
    "print(\"│   Space Complexity: O(T)    - store all elements               │\")\n",
    "print(\"│                                                                 │\")\n",
    "print(\"│   Pros: Massive parallelization on GPU/TPU                     │\")\n",
    "print(\"│   Cons: Only works for (semi-)linear recurrences               │\")\n",
    "print(\"└─────────────────────────────────────────────────────────────────┘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scan_visualization"
   },
   "outputs": [],
   "source": [
    "# Visualize the associative scan tree structure\n",
    "print(\"=\"*70)\n",
    "print(\"ASSOCIATIVE SCAN: TREE REDUCTION VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"For T=8 timesteps:\")\n",
    "print()\n",
    "print(\"Step 0 (T=8 leaves):\")\n",
    "print(\"  (A₀,b₀)  (A₁,b₁)  (A₂,b₂)  (A₃,b₃)  (A₄,b₄)  (A₅,b₅)  (A₆,b₆)  (A₇,b₇)\")\n",
    "print(\"      │        │        │        │        │        │        │        │\")\n",
    "print(\"      └───┬────┘        └───┬────┘        └───┬────┘        └───┬────┘\")\n",
    "print(\"          │                 │                 │                 │\")\n",
    "print(\"Step 1 (4 nodes):\")\n",
    "print(\"     (A₀₁,b₀₁)         (A₂₃,b₂₃)         (A₄₅,b₄₅)         (A₆₇,b₆₇)\")\n",
    "print(\"          │                 │                 │                 │\")\n",
    "print(\"          └────────┬────────┘                 └────────┬────────┘\")\n",
    "print(\"                   │                                   │\")\n",
    "print(\"Step 2 (2 nodes):\")\n",
    "print(\"            (A₀₁₂₃,b₀₁₂₃)                       (A₄₅₆₇,b₄₅₆₇)\")\n",
    "print(\"                   │                                   │\")\n",
    "print(\"                   └─────────────────┬─────────────────┘\")\n",
    "print(\"                                     │\")\n",
    "print(\"Step 3 (1 node = final result):\")\n",
    "print(\"                          (A₀₁₂₃₄₅₆₇,b₀₁₂₃₄₅₆₇)\")\n",
    "print()\n",
    "print(\"Total parallel steps: log₂(8) = 3  (vs 8 for sequential)\")\n",
    "print()\n",
    "print(\"Where composition is: (A₂,b₂) ∘ (A₁,b₁) = (A₂·A₁, A₂·b₁ + b₂)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload"
   },
   "source": [
    "---\n",
    "# Part III: Hands-On Analysis\n",
    "---\n",
    "\n",
    "## 4. Upload Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_checkpoint"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"Please upload checkpoint.pkl file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Load checkpoint\n",
    "with open('checkpoint.pkl', 'rb') as f:\n",
    "    ckpt = pickle.load(f)\n",
    "\n",
    "params = ckpt['params']\n",
    "print(f\"\\nLoaded checkpoint from epoch {ckpt.get('epoch', 'unknown')}\")\n",
    "print(f\"Top-level keys: {list(params.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "extract_params"
   },
   "outputs": [],
   "source": [
    "# Extract CSSM parameters for manual computation\n",
    "cssm_params = params['cssm_0']\n",
    "\n",
    "# Spatial kernels\n",
    "K_E = jnp.array(cssm_params['k_exc'])  # (C, k, k)\n",
    "K_I = jnp.array(cssm_params['k_inh'])  # (C, k, k)\n",
    "\n",
    "# Extract gate values (taking mean across spatial dimensions)\n",
    "def get_gate(name, activation='sigmoid'):\n",
    "    \"\"\"Extract gate value from parameters.\"\"\"\n",
    "    kernel = cssm_params[f'{name}_gate']['kernel']\n",
    "    bias = cssm_params[f'{name}_gate']['bias']\n",
    "    # Use mean for interpretability\n",
    "    val = kernel.mean() + bias.mean()\n",
    "    if activation == 'sigmoid':\n",
    "        return float(jax.nn.sigmoid(val))\n",
    "    elif activation == 'tanh':\n",
    "        return float(jnp.tanh(val) * 0.5)\n",
    "    return float(val)\n",
    "\n",
    "gates = {\n",
    "    'lambda_x': get_gate('decay_x'),\n",
    "    'lambda_y': get_gate('decay_y'),\n",
    "    'lambda_z': get_gate('decay_z'),\n",
    "    'alpha_E': get_gate('alpha_excit'),\n",
    "    'alpha_I': get_gate('alpha_inhib'),\n",
    "    'mu_E': get_gate('mu_excit'),\n",
    "    'mu_I': get_gate('mu_inhib'),\n",
    "    'gamma': get_gate('gamma', 'tanh'),\n",
    "    'delta': get_gate('delta', 'tanh'),\n",
    "}\n",
    "\n",
    "print(\"Extracted gate values:\")\n",
    "for name, val in gates.items():\n",
    "    print(f\"  {name:>10}: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_def"
   },
   "outputs": [],
   "source": [
    "# Also load the full model for comparison\n",
    "from src.models.simple_cssm import SimpleCSSM\n",
    "\n",
    "model = SimpleCSSM(\n",
    "    num_classes=2,\n",
    "    embed_dim=32,\n",
    "    depth=1,\n",
    "    cssm_type='hgru_bi',\n",
    "    kernel_size=15,\n",
    "    pos_embed='spatiotemporal',\n",
    "    seq_len=8,\n",
    ")\n",
    "print(\"Full model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_images"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "print(\"Upload a POSITIVE (connected) Pathfinder image:\")\n",
    "uploaded_pos = files.upload()\n",
    "pos_filename = list(uploaded_pos.keys())[0]\n",
    "pos_img = np.array(Image.open(io.BytesIO(uploaded_pos[pos_filename])).resize((224, 224)).convert('RGB')) / 255.0\n",
    "pos_img = pos_img.astype(np.float32)\n",
    "\n",
    "print(\"\\nUpload a NEGATIVE (disconnected) Pathfinder image:\")\n",
    "uploaded_neg = files.upload()\n",
    "neg_filename = list(uploaded_neg.keys())[0]\n",
    "neg_img = np.array(Image.open(io.BytesIO(uploaded_neg[neg_filename])).resize((224, 224)).convert('RGB')) / 255.0\n",
    "neg_img = neg_img.astype(np.float32)\n",
    "\n",
    "print(f\"\\nLoaded images: {pos_img.shape}, {neg_img.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "forward_viz"
   },
   "source": [
    "## 5. Step-by-Step Forward Pass Visualization\n",
    "\n",
    "Let's trace through the CSSM computation step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_sequential"
   },
   "outputs": [],
   "source": [
    "# Run sequential forward pass on positive example\n",
    "# First apply stem (conv layers) - simplified version\n",
    "def simple_stem(img):\n",
    "    \"\"\"Simplified stem: just downsample and project.\"\"\"\n",
    "    # In practice, this goes through conv layers\n",
    "    # Here we just resize for demonstration\n",
    "    from jax.image import resize\n",
    "    x = jnp.array(img)[None, ...]  # (1, H, W, C)\n",
    "    x = resize(x, (1, 56, 56, 3), method='bilinear')  # Downsample\n",
    "    # Project to embed_dim channels\n",
    "    proj = jnp.eye(3, 32)  # Simple projection\n",
    "    x = x @ proj  # (1, 56, 56, 32)\n",
    "    return x\n",
    "\n",
    "# Get input features\n",
    "U_pos = simple_stem(pos_img)\n",
    "U_neg = simple_stem(neg_img)\n",
    "print(f\"Input feature shape: {U_pos.shape}\")\n",
    "\n",
    "# For proper visualization, we need matching kernel size\n",
    "# Resize kernels if needed\n",
    "K_E_small = K_E[:, :11, :11]  # Use center 11x11\n",
    "K_I_small = K_I[:, :11, :11]\n",
    "\n",
    "print(f\"Kernel shape: {K_E_small.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_states"
   },
   "outputs": [],
   "source": [
    "# Run and visualize sequential forward pass\n",
    "print(\"Running sequential forward pass...\")\n",
    "\n",
    "# Simplified gates for visualization\n",
    "viz_gates = {\n",
    "    'lambda_x': 0.9, 'lambda_y': 0.9, 'lambda_z': 0.95,\n",
    "    'alpha_E': 0.3, 'alpha_I': 0.2,\n",
    "    'mu_E': 0.3, 'mu_I': 0.2,\n",
    "    'gamma': 0.1, 'delta': 0.1,\n",
    "}\n",
    "\n",
    "# Run forward pass\n",
    "X_hist, Y_hist, Z_hist = cssm_forward_sequential(\n",
    "    U_pos, K_E_small, K_I_small, viz_gates, T=8\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(X_hist)} state snapshots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_state_evolution"
   },
   "outputs": [],
   "source": [
    "# Visualize state evolution\n",
    "fig, axes = plt.subplots(3, 9, figsize=(18, 6))\n",
    "\n",
    "# Take mean across channels for visualization\n",
    "for t in range(9):\n",
    "    # X state\n",
    "    X_t = np.array(X_hist[t][0]).mean(axis=-1)  # (H, W)\n",
    "    im = axes[0, t].imshow(X_t, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "    axes[0, t].set_title(f't={t}', fontsize=10)\n",
    "    axes[0, t].axis('off')\n",
    "    if t == 0:\n",
    "        axes[0, t].set_ylabel('X (Excit)', fontsize=11)\n",
    "    \n",
    "    # Y state\n",
    "    Y_t = np.array(Y_hist[t][0]).mean(axis=-1)\n",
    "    axes[1, t].imshow(Y_t, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "    axes[1, t].axis('off')\n",
    "    if t == 0:\n",
    "        axes[1, t].set_ylabel('Y (Inhib)', fontsize=11)\n",
    "    \n",
    "    # Z state\n",
    "    Z_t = np.array(Z_hist[t][0]).mean(axis=-1)\n",
    "    axes[2, t].imshow(Z_t, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "    axes[2, t].axis('off')\n",
    "    if t == 0:\n",
    "        axes[2, t].set_ylabel('Z (Interact)', fontsize=11)\n",
    "\n",
    "plt.suptitle('CSSM State Evolution Over Time\\n(Red=positive, Blue=negative)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_receptive_field"
   },
   "outputs": [],
   "source": [
    "# Visualize receptive field growth\n",
    "# The effective receptive field grows with each timestep due to convolutions\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\"*70)\n",
    "print(\"RECEPTIVE FIELD GROWTH\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"At each timestep, the spatial convolutions K_E and K_I spread\")\n",
    "print(\"information from neighboring pixels. After T steps:\")\n",
    "print()\n",
    "print(\"  Effective receptive field ≈ T × kernel_size\")\n",
    "print()\n",
    "print(f\"  With kernel_size=15 and T=8:\")\n",
    "print(f\"    RF ≈ 8 × 15 = 120 pixels\")\n",
    "print()\n",
    "print(\"This is how CSSM can integrate information across the image:\")\n",
    "print(\"the contour endpoints can 'see' each other after enough timesteps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "temporal_grads"
   },
   "source": [
    "## 6. Temporal Gradient Attribution\n",
    "\n",
    "Which timesteps matter most for the decision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compute_grads"
   },
   "outputs": [],
   "source": [
    "def compute_temporal_gradients(img, target_class):\n",
    "    \"\"\"Compute gradient of decision w.r.t. input at each timestep.\"\"\"\n",
    "    x = jnp.array(img)[None, ...]\n",
    "    x_temporal = jnp.repeat(x[:, None, ...], 8, axis=1)\n",
    "    \n",
    "    def forward_fn(x_t):\n",
    "        logits = model.apply({'params': params}, x_t, training=False)\n",
    "        return logits[0, target_class]\n",
    "    \n",
    "    grads = jax.grad(forward_fn)(x_temporal)\n",
    "    grad_magnitude = jnp.abs(grads).sum(axis=(0, 2, 3, 4))\n",
    "    spatial_grads = jnp.abs(grads[0]).sum(axis=-1)\n",
    "    return grad_magnitude, spatial_grads\n",
    "\n",
    "pos_grad_mag, pos_spatial = compute_temporal_gradients(pos_img, 1)\n",
    "neg_grad_mag, neg_spatial = compute_temporal_gradients(neg_img, 0)\n",
    "\n",
    "print(\"Gradient magnitude per timestep:\")\n",
    "print(f\"  Positive: {np.array(pos_grad_mag).round(2)}\")\n",
    "print(f\"  Negative: {np.array(neg_grad_mag).round(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_temporal"
   },
   "outputs": [],
   "source": [
    "# Plot temporal importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart\n",
    "timesteps = np.arange(8)\n",
    "width = 0.35\n",
    "axes[0].bar(timesteps - width/2, np.array(pos_grad_mag), width, \n",
    "           label='Positive', color='green', alpha=0.7)\n",
    "axes[0].bar(timesteps + width/2, np.array(neg_grad_mag), width,\n",
    "           label='Negative', color='red', alpha=0.7)\n",
    "axes[0].set_xlabel('Timestep', fontsize=12)\n",
    "axes[0].set_ylabel('Gradient Magnitude', fontsize=12)\n",
    "axes[0].set_title('Which Timesteps Influence the Decision?', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].set_xticks(timesteps)\n",
    "\n",
    "# Cumulative importance\n",
    "pos_cumsum = np.cumsum(np.array(pos_grad_mag))\n",
    "neg_cumsum = np.cumsum(np.array(neg_grad_mag))\n",
    "axes[1].plot(timesteps, pos_cumsum / pos_cumsum[-1], 'g-o', label='Positive', linewidth=2)\n",
    "axes[1].plot(timesteps, neg_cumsum / neg_cumsum[-1], 'r-o', label='Negative', linewidth=2)\n",
    "axes[1].set_xlabel('Timestep', fontsize=12)\n",
    "axes[1].set_ylabel('Cumulative Importance (normalized)', fontsize=12)\n",
    "axes[1].set_title('Cumulative Information Integration', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].set_xticks(timesteps)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mechanism_attribution"
   },
   "source": [
    "## 7. Mechanism Attribution\n",
    "\n",
    "Which CSSM mechanisms drive the decision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "param_grads"
   },
   "outputs": [],
   "source": [
    "def parameter_gradients(img, target_class):\n",
    "    x = jnp.array(img)[None, ...]\n",
    "    x_temporal = jnp.repeat(x[:, None, ...], 8, axis=1)\n",
    "    \n",
    "    def loss_fn(p):\n",
    "        logits = model.apply({'params': p}, x_temporal, training=False)\n",
    "        return logits[0, target_class]\n",
    "    \n",
    "    return jax.grad(loss_fn)(params)\n",
    "\n",
    "pos_param_grads = parameter_gradients(pos_img, 1)\n",
    "neg_param_grads = parameter_gradients(neg_img, 0)\n",
    "\n",
    "# Summarize\n",
    "mechanisms = [\n",
    "    ('alpha_excit_gate', 'α_E (excitatory spread)', 'Spreads activation along contours'),\n",
    "    ('alpha_inhib_gate', 'α_I (inhibitory spread)', 'Suppresses off-contour activity'),\n",
    "    ('gamma_gate', 'γ (bilinear X*Z)', 'Attention-like context modulation'),\n",
    "    ('delta_gate', 'δ (Z update)', 'Accumulates E-I history'),\n",
    "    ('decay_x_gate', 'λ_x (X memory)', 'Retains excitatory state'),\n",
    "    ('decay_z_gate', 'λ_z (Z memory)', 'Retains interaction history'),\n",
    "]\n",
    "\n",
    "print(\"=\"*75)\n",
    "print(\"MECHANISM ATTRIBUTION\")\n",
    "print(\"=\"*75)\n",
    "print(f\"{'Mechanism':<25} {'Description':<35} {'Pos':>8} {'Neg':>8}\")\n",
    "print(\"-\"*75)\n",
    "\n",
    "for key, name, desc in mechanisms:\n",
    "    if key in pos_param_grads['cssm_0']:\n",
    "        pos_mag = np.abs(np.array(pos_param_grads['cssm_0'][key]['kernel'])).mean()\n",
    "        neg_mag = np.abs(np.array(neg_param_grads['cssm_0'][key]['kernel'])).mean()\n",
    "        print(f\"{name:<25} {desc:<35} {pos_mag:>8.5f} {neg_mag:>8.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "---\n",
    "# Part IV: Summary\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### 1. CSSM Equations\n",
    "\n",
    "$$X_{t+1} = \\lambda_x X_t + \\alpha_E (K_E * X_t) - \\alpha_I (K_I * Y_t) + \\gamma (X_t \\odot Z_t) + U$$\n",
    "$$Y_{t+1} = \\lambda_y Y_t + \\mu_E X_t - \\mu_I Y_t + U$$\n",
    "$$Z_{t+1} = \\lambda_z Z_t + \\delta (X_t - Y_t) + U$$\n",
    "\n",
    "### 2. Sequential vs Parallel\n",
    "\n",
    "| Aspect | Sequential | Parallel (Assoc. Scan) |\n",
    "|--------|------------|------------------------|\n",
    "| Time | O(T) | O(log T) |\n",
    "| Space | O(1) | O(T) |\n",
    "| Parallelism | None | Full |\n",
    "| Nonlinearity | Any | Linear/bilinear |\n",
    "\n",
    "### 3. Attention Analogy\n",
    "\n",
    "| Transformer | CSSM |\n",
    "|-------------|------|\n",
    "| Keys K | Z state (accumulated E-I) |\n",
    "| Queries Q | X state (current) |\n",
    "| Q·K attention | X ⊙ Z bilinear |\n",
    "| O(T²) | O(T) or O(log T) |\n",
    "\n",
    "### 4. Why CSSM Works for Pathfinder\n",
    "\n",
    "1. **Spatial kernels** spread activation along contours\n",
    "2. **E-I dynamics** enhance contrast (center-surround)\n",
    "3. **Temporal recurrence** grows receptive fields\n",
    "4. **Bilinear term** integrates context like attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"NOTEBOOK COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"You have learned:\")\n",
    "print(\"  1. The CSSM state equations (X, Y, Z dynamics)\")\n",
    "print(\"  2. How sequential vs parallel computation works\")\n",
    "print(\"  3. The associative scan algorithm for O(log T) recurrence\")\n",
    "print(\"  4. Why X*Z acts like growing attention\")\n",
    "print(\"  5. Which mechanisms drive contour integration\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
